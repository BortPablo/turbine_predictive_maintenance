{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(os.getenv('MLFLOW_URL'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_failures() -> pd.DataFrame:\n",
    "    \"\"\"Load the failures from the wind farm dataset.\"\"\"\n",
    "    df = pd.read_csv('../data/raw/htw-failures-2016.csv', sep=';')\n",
    "    aux = pd.read_csv('../data/raw/htw-failures-2017.csv', sep=';')\n",
    "\n",
    "    df = pd.concat([df, aux], axis=0).reset_index(drop=True)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df = df.set_index('Timestamp').sort_index()\n",
    "    df = df[df.Turbine_ID != 'T09']\n",
    "    df['Turbine_ID'] = df['Turbine_ID'].apply(lambda x: int(x[1:]))\n",
    "    return df\n",
    "\n",
    "def load_costs() -> pd.DataFrame:\n",
    "    \"\"\"Load the costs from the wind farm dataset.\"\"\"\n",
    "    return pd.read_csv('../data/raw/HTW_Costs.csv').set_index('Component')\n",
    "\n",
    "def trb_per_failures() -> dict:\n",
    "    \"\"\"Return a dictionary with the turbine ID per component failure\"\"\"\n",
    "    failures = load_failures()\n",
    "    trb_per_comp = {}\n",
    "    for comp in failures.Component.unique():\n",
    "        trb_per_comp[comp] = failures[(failures.Component == comp)&(failures.index >= '2017-06-01')].Turbine_ID.unique().tolist()\n",
    "    return trb_per_comp\n",
    "\n",
    "def create_time_columns(df) -> pd.DataFrame:\n",
    "    \"\"\"Create time columns from the index of the dataframe.\"\"\"\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    return df\n",
    "\n",
    "def filter_FP(FP, resample_time) -> list:\n",
    "    \"\"\"Filter the consecutive false positives from the list of FP.\"\"\"\n",
    "    aux = []\n",
    "    for i in range(len(FP)-1):\n",
    "        if (FP[i+1]-FP[i]).total_seconds() == 60*60*resample_time:\n",
    "            continue\n",
    "        aux.append(FP[i])\n",
    "        aux.append(FP[i+1])\n",
    "    if len(aux) == 0:\n",
    "        aux.append(FP[0])\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = load_failures()\n",
    "failure_dates = failures[(failures['Turbine_ID'] == 7) & (failures['Component'] == 'GENERATOR_BEARING') & (failures.index > '2017-06-01')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(resample_time, center, deviation, num_dev, xgb_params) -> None:\n",
    "    \"\"\" Function to test the parameters of the model\"\"\"\n",
    "    targets = {\n",
    "    'GENERATOR': ['Gen_Phase1_Temp_Avg','Gen_Phase2_Temp_Avg','Gen_Phase3_Temp_Avg','Gen_SlipRing_Temp_Avg',],\n",
    "    'HYDRAULIC_GROUP': ['Hyd_Oil_Temp_Avg'],\n",
    "    'GENERATOR_BEARING': ['Gen_Bear_Temp_Avg','Gen_Bear2_Temp_Avg'],\n",
    "    'TRANSFORMER': ['HVTrafo_Phase1_Temp_Avg','HVTrafo_Phase2_Temp_Avg','HVTrafo_Phase3_Temp_Avg'],\n",
    "    'GEARBOX': ['Gear_Oil_Temp_Avg', 'Gear_Bear_Temp_Avg']\n",
    "    }\n",
    "\n",
    "    features = ['Gen_RPM_Avg', 'Nac_Temp_Avg','Rtr_RPM_Avg', 'Amb_WindSpeed_Avg', 'Amb_WindDir_Abs_Avg', 'Amb_Temp_Avg', 'Prod_LatestAvg_TotActPwr', 'Prod_LatestAvg_TotReactPwr',\n",
    "        'Spin_Temp_Avg', 'Blds_PitchAngle_Avg', 'Grd_Busbar_Temp_Avg','Nac_Direction_Avg', 'theoretical_performance_ratio']\n",
    "    \n",
    "    for id in [1, 6, 7, 11]:\n",
    "        for comp in targets.keys():\n",
    "            if id not in trb_per_failures()[comp]:\n",
    "                continue\n",
    "            for col in targets[comp]:\n",
    "                MODULE_NAME = os.getenv('MODULE_NAME')\n",
    "                # Start mlflow run\n",
    "                with mlflow.start_run(run_name=f'{MODULE_NAME}_trb{id}_{col}') as run:\n",
    "                    # Set tags\n",
    "                    mlflow.set_tags({\n",
    "                        'MODULE': MODULE_NAME,\n",
    "                        'trb_num': id,\n",
    "                        'col': col,\n",
    "                        'model': 'XGBRegressor'\n",
    "                    })\n",
    "\n",
    "                    # Load train/test datasets\n",
    "                    train = pd.read_parquet(f'../data/processed/{id}/train.parquet')\n",
    "\n",
    "                    # Select the target variable\n",
    "                    y_train = train.pop(col)\n",
    "\n",
    "                    # Select the features to be used in the model\n",
    "                    train = train[features]\n",
    "                    train = create_time_columns(train)\n",
    "\n",
    "                    # Split the train dataset into train and validation\n",
    "                    X_train, X_val, y_train, y_val = train_test_split(train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "                    # Scale the features\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "                    # Select the parameters for XGBRegressor model\n",
    "                    mlflow.log_params(xgb_params)\n",
    "                    mlflow.log_params({\n",
    "                        'resample_time': resample_time,\n",
    "                        'center': center,\n",
    "                        'deviation': deviation,\n",
    "                        'num_dev': num_dev\n",
    "                    })\n",
    "\n",
    "                    # Train the model\n",
    "                    model = XGBRegressor(**xgb_params)\n",
    "                    model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "                    # Save the scaler and model into mflow\n",
    "                    mlflow.sklearn.log_model(scaler, 'scaler')\n",
    "                    mlflow.xgboost.log_model(model, 'model')\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    y_pred = model.predict(X_val_scaled)\n",
    "                    mlflow.log_metrics({\n",
    "                        'rmse': np.sqrt(mean_squared_error(y_val, y_pred)),\n",
    "                        'mae': mean_absolute_error(y_val, y_pred),\n",
    "                        'r2': r2_score(y_val, y_pred)\n",
    "                    })\n",
    "\n",
    "                    X_val = pd.DataFrame(X_val, columns=features + ['month', 'day', 'hour', 'dayofweek'])\n",
    "                    X_val['y_true'] = y_val\n",
    "                    X_val['y_pred'] = y_pred\n",
    "                    X_val['residual'] = y_val - y_pred\n",
    "\n",
    "                    threshold = {\n",
    "                        'mean': str(X_val['residual'].mean()),\n",
    "                        'median': str(X_val['residual'].median()),\n",
    "                        'std': str(X_val['residual'].std()),\n",
    "                        'mad': str((X_val['residual'] - X_val['residual'].median()).abs().median())\n",
    "                    }\n",
    "                    # Save json\n",
    "                    with open(f'../data/interim/threshold.json', 'w') as f:\n",
    "                        json.dump(threshold, f)\n",
    "\n",
    "                    mlflow.log_artifact(f'../data/interim/threshold.json')\n",
    "\n",
    "                    # Get costs metric\n",
    "                    test = pd.read_parquet(f'../data/processed/{id}/test.parquet')\n",
    "\n",
    "                    y_test = test.pop(col)\n",
    "\n",
    "                    # Select the features to be used in the model\n",
    "                    test = test[features]\n",
    "                    test = create_time_columns(test)\n",
    "\n",
    "                    X_test_scaled = scaler.transform(test)\n",
    "\n",
    "                    test[col] = y_test\n",
    "                    test['y_pred'] = model.predict(X_test_scaled)\n",
    "                    test['residual'] = test[col] - test['y_pred']\n",
    "\n",
    "                    test = test[[col, 'y_pred', 'residual']]\n",
    "\n",
    "                    alarms = test.copy()\n",
    "                    alarms = alarms.resample(f'{resample_time}H').mean()\n",
    "                    alarms['anomalous_pred'] = np.where(\n",
    "                        (alarms['residual'] >=  float(threshold[center]) + num_dev * float(threshold[deviation]))|\n",
    "                        (alarms['residual'] <=  float(threshold[center]) - num_dev * float(threshold[deviation])),\n",
    "                        1,\n",
    "                        0\n",
    "                    )\n",
    "                    failures = load_failures()\n",
    "                    failure_dates = failures[(failures['Turbine_ID'] == id) & (failures['Component'] == comp) & (failures.index >= '2017-06-01')].index\n",
    "\n",
    "                    init = True\n",
    "                    for date in failure_dates:\n",
    "                        if init:\n",
    "                            alarms['anomalous_real'] = np.where(\n",
    "                                (alarms.index >= date - pd.Timedelta(days=90)) & (alarms.index <= date),\n",
    "                                1,\n",
    "                                0\n",
    "                            )\n",
    "                            init = False\n",
    "                        else:\n",
    "                            alarms['anomalous_real'] = np.where(\n",
    "                                (alarms.index >= date - pd.Timedelta(days=90)) & (alarms.index <= date),\n",
    "                                1,\n",
    "                                alarms['anomalous_real']\n",
    "                            )\n",
    "\n",
    "                    alarms[['anomalous_real', 'anomalous_pred']].plot(figsize=(15, 5), title=f'Anomalous Real vs Anomalous Predicted (turbine {id}, column {col})')\n",
    "                    for date in failure_dates:\n",
    "                        plt.axvline(x=date, color='red', linestyle='--')\n",
    "                    plt.savefig(f'../reports/check.png')\n",
    "                    mlflow.log_artifact(f'../reports/check.png')\n",
    "\n",
    "                    costs = load_costs()\n",
    "\n",
    "                    total_cost = 0\n",
    "                    TP_cost = 0\n",
    "                    FN_cost = 0\n",
    "                    TP = alarms[(alarms['anomalous_real']&alarms['anomalous_pred']) == 1].index\n",
    "                    if TP.shape[0] == 0:\n",
    "                        FN_cost -= costs.loc[comp, 'Replacement_Cost'] * failure_dates.shape[0]\n",
    "                        total_cost -= costs.loc[comp, 'Replacement_Cost'] * failure_dates.shape[0]\n",
    "                    else:\n",
    "                        for date in failure_dates:\n",
    "                            aux = TP[(TP < date)&(TP > date - pd.Timedelta(days=90))]\n",
    "                            if len(aux) == 0:\n",
    "                                total_cost -= costs.loc[comp, 'Replacement_Cost']\n",
    "                                FN_cost -= costs.loc[comp, 'Replacement_Cost']\n",
    "                                continue\n",
    "                            days = (date - aux[0]).days + (date - aux[0]).seconds/(24*60*60)\n",
    "                            total_cost += costs.loc[comp, 'Replacement_Cost'] - (costs.loc[comp, 'Repair_Cost'] + (costs.loc[comp, 'Replacement_Cost'] - costs.loc[comp, 'Repair_Cost'])*(1 - days/90))\n",
    "                            TP_cost += costs.loc[comp, 'Replacement_Cost'] - (costs.loc[comp, 'Repair_Cost'] + (costs.loc[comp, 'Replacement_Cost'] - costs.loc[comp, 'Repair_Cost'])*(1 - days/90))\n",
    "\n",
    "                    FP = alarms[(alarms['anomalous_real'] == 0)&(alarms['anomalous_pred'] == 1)].index\n",
    "                    if len(FP) > 1:\n",
    "                        FP = filter_FP(FP, resample_time)\n",
    "                    FP_cost = -len(FP) * costs.loc[comp, 'Inspection_cost']\n",
    "                    total_cost += FP_cost\n",
    "\n",
    "                    mlflow.log_metrics({\n",
    "                        'total_cost': total_cost,\n",
    "                        'TP_cost': TP_cost,\n",
    "                        'FP_cost': FP_cost,\n",
    "                        'FN_cost': FN_cost\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in [200, 500, 1000]:\n",
    "    for max_depth in [3, 5, 7, 9]:\n",
    "        for min_child_weight in [1, 3, 5]:\n",
    "            for learning_rate in [0.01, 0.05, 0.1]:\n",
    "                for resample_time in [6, 12]:\n",
    "                    for center in ['median']:\n",
    "                        for deviation in ['std', 'mad']:\n",
    "                            for num_dev in [3]:                              \n",
    "\n",
    "                                xgb_params = {\n",
    "                                    'n_estimators': n_estimators,\n",
    "                                    'max_depth': max_depth,\n",
    "                                    'min_child_weight': min_child_weight,\n",
    "                                    'learning_rate': learning_rate,\n",
    "                                }\n",
    "                                test_params(\n",
    "                                    resample_time=resample_time,\n",
    "                                    center=center,\n",
    "                                    deviation=deviation,\n",
    "                                    num_dev=num_dev,\n",
    "                                    xgb_params=xgb_params\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
